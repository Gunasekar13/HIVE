Scenario Based questions:
-------- ----- ---------
1. Will the reducer work or not if you use “Limit 1” in any HiveQL query?
      
      It depends on the query.
 
 2. Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration.
 Then, what will happen if we have multiple clients trying to access Hive at the same time? 

       Default metastore configuration was standalone.It can be access only one clients. It cann't able to share across the cluster. If trying to access
multiple client it can be error.

3. Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

      Solution for this case 'Partitioning'. Create the partition based on the month.
      # create partition table 
         hive> create table if not exists transaction_partition_details(
             > cust_id int,
             > amount float,
             > country string
             > )
             > partitioned by (month string)
             > row format delimited
             > fields terminated by ','
             > ;
       # enable the dynamic partition 
         hive> SET hive.exec.dynamic.partition.mode = nonstrict;
       # load the data to the partition table 
         hive> insert overwrite table transaction_partition_details partition (month) select *from transaction_details;
       # Query the data
          Query will executed eaach month and past.
 
4. How can you add a new partition for the month December in the above partitioned table?
      
     hive> Alter table transaction_partition_details add partition (month='DEC') LOCATION 'hdfs://localhost/user/hive/warehouse/transaction_partition_details';

5. I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column.
How will you remove this error?
      For remove this error following comments,
      SET hive.exec.dynamic.partition = true;
      hive> SET hive.exec.dynamic.partition.mode = nonstrict;

6. Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
hive>create table sample_csv_table(
    > id int,
    > first_name string,
    > last_name string,
    > email varchar(20),
    > gender string,
    > ip_address string
    > )
    > row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > with serdeproperties(
    > "separatorChar"=",",
    > "quoteChar"="\"",
    > "escapeChar"="\\"
    > )
    > stored as textfile location 'hdfs://localhost/temp'
    > tblproperties('skip.header.line.count'='1');
      
7. Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files.
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
         Solution for this case is use 'SEQUENTIALFILE',
         hive> create table sample_table (
             > id int,
             > name string,
             > email string,
             > country string
             > )
             > row format delimited
             > fields terminated by ','
             > tblproperties('skip.header.line.count'='1');
          # load the file and create as seqfile
            hive> create table seq_table(
                > id int,
                > name string,
                > email string,
                > country string
                > )
                > row format delimited
                > fields terminated by ','
                > stored as SEQUENCEFILE
                > tblproperties('skip.header.line.count'='1');
         # load the file
             hive> insert overwrite table seq_table select * from sample_table;

8. LOAD DATA LOCAL INPATH ‘Home/country/state/’ OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?
      Hive local inpath should contain a file only not directory.
 
9. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

       Yes, It is possible. follow the below step.
       step 1: take  a new system, create username and password
       step 2: install SSH and with master node setup SSH connections 
       step 3: Add ssh public_rsa id key to authorized keys file
       step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:
               192.168.1.102 slave3.in slave3
       Step 5: Start the DataNode on a new node
       Step 6: Login to the new node like suhadoop or:
               ssh -X hadoop@192.168.1.103
       Step 7: Start HDFS of the newly added slave node by using the following command:
               ./bin/hadoop-daemon.sh start data node
_____________________________________________________________________________________________________________________________              
Hive Practical questions:
---- --------- ---------
Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

hive> Create table CUSTOMERS(
    > ID int,
    > NAME string,
    > AGE int,
    > ADDRESS string,
    > SALARY int
    > )
    > row format delimited
    > fields terminated by ','
    > tblproperties('skip.header.line.count'='1');
 
hive> load data local inpath '/config/workspace/customer.txt' into table CUSTOMERS;
1       Guna    23      chennai 23000
2       Anu     22      coimbatore      23000
3       Amirtha 24      Banglore        26000
4       Aishu   30      Hyderapad       30000
5       Arun    26      chennai 25000
6       Dilli   28      coimbatore      27000
7       Rolex   25      Hyderapad       30000


hive> Create table ORDERS(
    > OID int,DATES string,
    > CUSTOMER_ID int,
    > AMOUNT int
    > )
    > row format delimited
    > fields terminated by ','
    > tblproperties('skip.header.line.count'='1'
    > );
    
hive> Load data local inpath '/config/workspace/Order.txt' into table ORDERS;
10034   2022-02-15      3       400
13423   2022-06-12      2       500
12452   2022-09-11      4       1000
13423   2022-04-17      1       700
12456   2022-05-12      7       350
19874   2022-01-10      6       680
14732   2022-02-23      5       890

✍Inner JOIN :
    hive> select *from customers c inner join orders o
    > on c.id=o.cust_id;
    
✍LEFT JOIN :
    hive> select *from customers c LEFT JOIN orders o
    > on c.id=o.cust_id;
    
✍RIGHT JOIN :  
    hive> select *from customers c RIGHT JOIN orders o
    > on c.id=o.cust_id;
    
✍FULL OUTER JOIN :
    hive> select *from customers c FULL OUTER JOIN orders o
    > on c.id=o.cust_id;
        

BUILD A DATA PIPELINE WITH HIVE




             
            
            
            
            
            
            
            
